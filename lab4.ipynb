{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Laboratorium 7\n","\n","Celem siĂłdmego laboratorium jest zapoznanie siÄ oraz zaimplementowanie algorytmu gĹÄbokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm bÄdzie testowany z wykorzystaniem Ĺrodowiska z OpenAI - _CartPole_.\n"]},{"cell_type":"markdown","metadata":{},"source":["DoĹÄczenie standardowych bibliotek\n"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'gym'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7a828ff9d16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"]}],"source":["from collections import deque\n","import gym\n","import numpy as np\n","import random"]},{"cell_type":"markdown","metadata":{},"source":["DoĹÄczenie bibliotek do obsĹugi sieci neuronowych\n"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{},"source":["## Zadanie 1 - Actor-Critic\n","\n","<p style='text-align: justify;'>\n","Celem Äwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu naleĹźy utworzyÄ dwie gĹÄbokie sieci neuronowe:\n","    1. *actor* - sieÄ, ktĂłra bÄdzie uczyĹa siÄ optymalnej strategii (podobna do tej z laboratorium 6),\n","    2. *critic* - sieÄ, ktĂłra bÄdzie uczyĹa siÄ funkcji oceny stanu (podobnie jak siÄ DQN).\n","Wagi sieci *actor* aktualizowane sÄ zgodnie ze wzorem:\n","\\begin{equation*}\n","    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n","\\end{equation*}\n","Wagi sieci *critic* aktualizowane sÄ zgodnie ze wzorem:\n","\\begin{equation*}\n","    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n","\\end{equation*}\n","gdzie:\n","\\begin{equation*}\n","    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n","\\end{equation*}\n","</p>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class REINFORCEAgent:\n","    def __init__(self, state_size, action_size, actor, critic):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.gamma = 0.99    # discount rate\n","        self.learning_rate = 0.001\n","        self.actor = actor\n","        self.critic = critic #critic network should have only one output\n","\n","\n","    def get_action(self, state):\n","        \"\"\"\n","        Compute the action to take in the current state, basing on policy returned by the network.\n","\n","        Note: To pick action according to the probability generated by the network\n","        \"\"\"\n","\n","        #\n","        # INSERT CODE HERE to get action in a given state\n","        #        \n","        \n","        return chosen_action\n","\n","  \n","\n","    def learn(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Function learn networks using information about state, action, reward and next state. \n","        First the values for state and next_state should be estimated based on output of critic network.\n","        Critic network should be trained based on target value:\n","        target = r + \\gamma next_state_value if not done]\n","        target = r if done.\n","        Actor network shpuld be trained based on delta value:\n","        delta = target - state_value\n","        \"\"\"\n","        #\n","        # INSERT CODE HERE to train network\n","        #\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["Czas przygotowaÄ model sieci, ktĂłra bÄdzie siÄ uczyĹa dziaĹania w Ĺrodowisku [_CartPool_](https://gym.openai.com/envs/CartPole-v0/):\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["env = gym.make(\"CartPole-v0\").env\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","alpha_learning_rate = 0.0001\n","beta_learning_rate = 0.0005\n","\n","class ActorCriticModel(nn.Module):\n","    def __init__(self, action_size, state_size) -> None:\n","        super().__init__()\n","        self.lin1 =  nn.Linear(action_size, 32)\n","        self.lin2_1 = nn.Linear(32, state_size)\n","        self.lin2_2 = nn.Linear(32, state_size)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.lin1(x))\n","        out1 = self.lin2_1(x)\n","        out2 = self.lin2_2(x)\n","        return out1, out2"]},{"cell_type":"markdown","metadata":{},"source":["Czas nauczyÄ agenta gry w Ĺrodowisku _CartPool_:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["agent = Agent(state_size, action_size, actor_model, critic_model)\n","\n","\n","for i in range(100):\n","    score_history = []\n","\n","    for i in range(100):\n","        done = False\n","        score = 0\n","        state = env.reset()\n","        while not done:\n","            action = agent.choose_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","            agent.learn(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","        score_history.append(score)\n","\n","    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n","\n","    if np.mean(score_history) > 300:\n","        print(\"You Win!\")\n","        break"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
