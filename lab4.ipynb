{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Laboratorium 7\n","\n","Celem siĂłdmego laboratorium jest zapoznanie siÄ oraz zaimplementowanie algorytmu gĹÄbokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm bÄdzie testowany z wykorzystaniem Ĺrodowiska z OpenAI - _CartPole_.\n"]},{"cell_type":"markdown","metadata":{},"source":["DoĹÄczenie standardowych bibliotek\n"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["from collections import deque\n","import gym\n","import numpy as np\n","import random"]},{"cell_type":"markdown","metadata":{},"source":["DoĹÄczenie bibliotek do obsĹugi sieci neuronowych\n"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam"]},{"cell_type":"markdown","metadata":{},"source":["## Zadanie 1 - Actor-Critic\n","\n","Celem Äwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu naleĹźy utworzyÄ dwie gĹÄbokie sieci neuronowe: 1. _actor_ - sieÄ, ktĂłra bÄdzie uczyĹa siÄ optymalnej strategii (podobna do tej z laboratorium 6), 2. _critic_ - sieÄ, ktĂłra bÄdzie uczyĹa siÄ funkcji oceny stanu (podobnie jak siÄ DQN).\n","Wagi sieci _actor_ aktualizowane sÄ zgodnie ze wzorem:\n","\n","\\begin{equation*}\n","\\theta \\leftarrow \\theta + \\alpha \\delta*t \\nabla*\\theta log \\pi\\_{\\theta}(a_t, s_t | \\theta).\n","\\end{equation*}\n","\n","Wagi sieci _critic_ aktualizowane sÄ zgodnie ze wzorem:\n","\\begin{equation*}\n","w \\leftarrow w + \\beta \\delta*t \\nabla_w\\upsilon(s*{t + 1}, w),\n","\\end{equation*}\n","gdzie:\n","\\begin{equation*}\n","\\delta*t \\leftarrow r_t + \\gamma \\upsilon(s*{t + 1}, w) - \\upsilon(s_t, w).\n","\\end{equation*}\n"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["class Agent:\n","    def __init__(self, state_size, action_size, model, alpha_learning_rate):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.gamma = 0.99    # discount rate\n","        self.model = model\n","        self.optimizer = Adam(self.model.parameters(), alpha_learning_rate)\n","        # self.critic_optimizer = Adam(self.critic.parameters(), beta_learning_rate)\n","\n","\n","    def get_action(self, state):\n","        state = torch.tensor(state, dtype=torch.float32)\n","        with torch.no_grad():\n","            probs = torch.log_softmax(self.model(state)[0], dim=-1)\n","        return torch.multinomial(probs.exp(), 1).item()\n","\n","    def learn(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Function learn networks using information about state, action, reward and next state. \n","        First the values for state and next_state should be estimated based on output of critic network.\n","        Critic network should be trained based on target value:\n","        target = r + \\gamma next_state_value if not done]\n","        target = r if done.\n","        Actor network shpuld be trained based on delta value:\n","        delta = target - state_value\n","        \"\"\"\n","        #\n","        # INSERT CODE HERE to train network\n","        #\n","        state = torch.tensor(state, dtype=torch.float32)\n","        action = torch.tensor(action, dtype=torch.int64).unsqueeze(0)\n","        reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","        done = torch.tensor(done, dtype=torch.float32).unsqueeze(0)\n","\n","        def get_delta():\n","            next_value = self.model(next_state)[1].detach()\n","            return reward + self.gamma * next_value * (1 - done) - self.model(state)[1]\n","\n","        actor_loss = -(get_delta() * torch.log_softmax(self.model(state)[0], dim=-1)[action])\n","        critic_loss = get_delta().pow(2).mean()\n","\n","        actor_loss.backward(retain_graph=True)\n","        critic_loss.backward()\n","        self.optimizer.step()\n","\n","        self.optimizer.zero_grad()\n","\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["Czas przygotowaÄ model sieci, ktĂłra bÄdzie siÄ uczyĹa dziaĹania w Ĺrodowisku [_CartPool_](https://gym.openai.com/envs/CartPole-v0/):\n"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/ml/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n"]}],"source":["env = gym.make(\"CartPole-v0\").env\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","alpha_learning_rate = 0.0001*2\n","\n","    \n","actor_model = nn.Sequential(\n","    nn.Linear(state_size, 32),\n","    nn.ReLU(),\n","    nn.Linear(32, action_size)\n",")\n","\n","critic_model = nn.Sequential(\n","    nn.Linear(state_size, 16),\n","    nn.ReLU(),\n","    nn.Linear(16, 1)\n",")\n","\n","class ActorCriticModel(nn.Module):\n","    def __init__(self, state_size, action_size) -> None:\n","        super().__init__()\n","        self.backbone = nn.Sequential(nn.Linear(state_size, 32), nn.ReLU())\n","        self.actor = nn.Sequential(nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, action_size))\n","        self.critic = nn.Sequential(nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 1))\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        return self.actor(x), self.critic(x)\n","    \n","actor_critic_model = ActorCriticModel(state_size, action_size)"]},{"cell_type":"markdown","metadata":{},"source":["Czas nauczyÄ agenta gry w Ĺrodowisku _CartPool_:\n"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/ml/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"name":"stdout","output_type":"stream","text":["mean reward:14.830\n","mean reward:10.330\n","mean reward:9.980\n","mean reward:10.370\n","mean reward:13.550\n","mean reward:12.640\n","mean reward:16.910\n","mean reward:18.880\n","mean reward:21.870\n","mean reward:24.870\n","mean reward:33.770\n","mean reward:37.530\n","mean reward:43.040\n","mean reward:52.430\n","mean reward:88.790\n","mean reward:105.040\n","mean reward:110.820\n","mean reward:134.580\n","mean reward:332.730\n","You Win!\n"]}],"source":["agent = Agent(state_size, action_size, actor_critic_model, alpha_learning_rate)\n","\n","\n","for i in range(100):\n","    score_history = []\n","\n","    for i in range(100):\n","        done = False\n","        score = 0\n","        state = env.reset()\n","        state = state[0]\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _, _ = env.step(action)\n","            agent.learn(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","        score_history.append(score)\n","\n","    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n","\n","    if np.mean(score_history) > 300:\n","        print(\"You Win!\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
