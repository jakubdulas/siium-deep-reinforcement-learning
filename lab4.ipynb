{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Laboratorium 7\n","\n","Celem siĂłdmego laboratorium jest zapoznanie siÄ oraz zaimplementowanie algorytmu gĹÄbokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm bÄdzie testowany z wykorzystaniem Ĺrodowiska z OpenAI - _CartPole_.\n"]},{"cell_type":"markdown","metadata":{},"source":["DoĹÄczenie standardowych bibliotek\n"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["from collections import deque\n","import gym\n","import numpy as np\n","import random"]},{"cell_type":"markdown","metadata":{},"source":["DoĹÄczenie bibliotek do obsĹugi sieci neuronowych\n"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam"]},{"cell_type":"markdown","metadata":{},"source":["## Zadanie 1 - Actor-Critic\n","\n","Celem Äwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu naleĹźy utworzyÄ dwie gĹÄbokie sieci neuronowe: 1. _actor_ - sieÄ, ktĂłra bÄdzie uczyĹa siÄ optymalnej strategii (podobna do tej z laboratorium 6), 2. _critic_ - sieÄ, ktĂłra bÄdzie uczyĹa siÄ funkcji oceny stanu (podobnie jak siÄ DQN).\n","Wagi sieci _actor_ aktualizowane sÄ zgodnie ze wzorem:\n","\n","\\begin{equation*}\n","\\theta \\leftarrow \\theta + \\alpha \\delta*t \\nabla*\\theta log \\pi\\_{\\theta}(a_t, s_t | \\theta).\n","\\end{equation*}\n","\n","Wagi sieci _critic_ aktualizowane sÄ zgodnie ze wzorem:\n","\\begin{equation*}\n","w \\leftarrow w + \\beta \\delta*t \\nabla_w\\upsilon(s*{t + 1}, w),\n","\\end{equation*}\n","gdzie:\n","\\begin{equation*}\n","\\delta*t \\leftarrow r_t + \\gamma \\upsilon(s*{t + 1}, w) - \\upsilon(s_t, w).\n","\\end{equation*}\n"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["class Agent:\n","    def __init__(self, state_size, action_size, actor, critic, alpha_learning_rate, beta_learning_rate):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.gamma = 0.99    # discount rate\n","        self.actor = actor\n","        self.critic = critic #critic network should have only one output\n","        self.actor_optimizer = Adam(self.actor.parameters(), alpha_learning_rate)\n","        self.critic_optimizer = Adam(self.critic.parameters(), beta_learning_rate)\n","\n","\n","    def get_action(self, state):\n","        state = torch.tensor(state, dtype=torch.float32)\n","        with torch.no_grad():\n","            probs = torch.log_softmax(self.actor(state), dim=-1)\n","        return torch.multinomial(probs.exp(), 1).item()\n","\n","    def learn(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Function learn networks using information about state, action, reward and next state. \n","        First the values for state and next_state should be estimated based on output of critic network.\n","        Critic network should be trained based on target value:\n","        target = r + \\gamma next_state_value if not done]\n","        target = r if done.\n","        Actor network shpuld be trained based on delta value:\n","        delta = target - state_value\n","        \"\"\"\n","        #\n","        # INSERT CODE HERE to train network\n","        #\n","        state = torch.tensor(state, dtype=torch.float32)\n","        action = torch.tensor(action, dtype=torch.int64).unsqueeze(0)\n","        reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","        done = torch.tensor(done, dtype=torch.float32).unsqueeze(0)\n","\n","        def get_delta():\n","            next_value = self.critic(next_state).detach()\n","            return reward + self.gamma * next_value * (1 - done) - self.critic(state)\n","\n","        actor_loss = -(get_delta() * torch.log_softmax(self.actor(state), dim=-1)[action])\n","        critic_loss = get_delta().pow(2).mean()\n","\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["Czas przygotowaÄ model sieci, ktĂłra bÄdzie siÄ uczyĹa dziaĹania w Ĺrodowisku [_CartPool_](https://gym.openai.com/envs/CartPole-v0/):\n"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[],"source":["env = gym.make(\"CartPole-v0\").env\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","alpha_learning_rate = 0.0001*2\n","beta_learning_rate = 0.0005*2\n","\n","    \n","actor_model = nn.Sequential(\n","    nn.Linear(state_size, 32),\n","    nn.ReLU(),\n","    nn.Linear(32, action_size)\n",")\n","\n","critic_model = nn.Sequential(\n","    nn.Linear(state_size, 16),\n","    nn.ReLU(),\n","    nn.Linear(16, 1)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Czas nauczyÄ agenta gry w Ĺrodowisku _CartPool_:\n"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mean reward:18.140\n","mean reward:18.290\n","mean reward:22.750\n","mean reward:20.650\n","mean reward:23.780\n","mean reward:25.130\n","mean reward:27.390\n","mean reward:25.580\n","mean reward:29.430\n","mean reward:33.450\n","mean reward:39.720\n","mean reward:45.720\n","mean reward:52.030\n","mean reward:55.570\n","mean reward:61.900\n","mean reward:102.900\n","mean reward:90.240\n","mean reward:78.330\n","mean reward:176.450\n","mean reward:194.680\n","mean reward:75.930\n","mean reward:162.280\n","mean reward:159.740\n","mean reward:221.560\n","mean reward:250.870\n","mean reward:158.710\n","mean reward:166.260\n","mean reward:152.000\n","mean reward:150.720\n","mean reward:1095.120\n","You Win!\n"]}],"source":["agent = Agent(state_size, action_size, actor_model, critic_model, alpha_learning_rate, beta_learning_rate)\n","\n","\n","for i in range(100):\n","    score_history = []\n","\n","    for i in range(100):\n","        done = False\n","        score = 0\n","        state = env.reset()\n","        state = state[0]\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _, _ = env.step(action)\n","            agent.learn(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","        score_history.append(score)\n","\n","    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n","\n","    if np.mean(score_history) > 300:\n","        print(\"You Win!\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
