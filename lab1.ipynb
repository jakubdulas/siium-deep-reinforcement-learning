{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Laboratorium 4 (4 pkt.)\n",
                "\n",
                "Celem czwartego laboratorium jest zapoznanie siÄ oraz zaimplementowanie algorytmĂłw gĹÄbokiego uczenia aktywnego. Zaimplementowane algorytmy bÄdÄ testowane z wykorzystaniem wczeĹniej przygotowanych Ĺrodowisk: _FrozenLake_ i _Pacman_ oraz Ĺrodowiska z OpenAI - _CartPole_.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "DoĹÄczenie standardowych bibliotek\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import deque\n",
                "import gym\n",
                "import numpy as np\n",
                "import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "DoĹÄczenie bibliotek ze Ĺrodowiskami:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "from env.FrozenLakeMDP import frozenLake\n",
                "from env.FrozenLakeMDPExtended import frozenLake as frozenLakeExtended"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "DoĹÄczenie bibliotek do obsĹugi sieci neuronowych\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.optim import Adam\n",
                "import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Zadanie 1 - Deep Q-Network\n",
                "\n",
                "<p style='text-align: justify;'>\n",
                "Celem Äwiczenie jest zaimplementowanie algorytmu Deep Q-Network. WartosciÄ oczekiwanÄ sieci jest:\n",
                "\\begin{equation}\n",
                "        Q(s_t, a_t) = r_{t+1} + \\gamma \\text{max}_a Q(s_{t + 1}, a)\n",
                "\\end{equation}\n",
                "</p>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DQNAgent:\n",
                "    def __init__(self, action_size, learning_rate, model: nn.Sequential):\n",
                "        self.action_size = action_size\n",
                "        self.memory = deque(maxlen=2000)\n",
                "        self.gamma = 0.90    # discount rate\n",
                "        self.epsilon = 0.8  # exploration rate\n",
                "        self.epsilon_min = 0.01\n",
                "        self.epsilon_decay = 0.9\n",
                "        self.learning_rate = learning_rate\n",
                "        self.model = model\n",
                "        self.loss_fn = nn.MSELoss()\n",
                "        self.optimizer = Adam(self.model.parameters(), self.learning_rate)\n",
                "\n",
                "    def remember(self, state, action, reward, next_state, done):\n",
                "        #Function adds information to the memory about last action and its results\n",
                "        self.memory.append((state, action, reward, next_state, done)) \n",
                "\n",
                "    def get_action(self, state):\n",
                "        \"\"\"\n",
                "        Compute the action to take in the current state, including exploration.\n",
                "        With probability self.epsilon, we should take a random action.\n",
                "            otherwise - the best policy action (self.get_best_action).\n",
                "\n",
                "        Note: To pick randomly from a list, use random.choice(list).\n",
                "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
                "              and compare it with your probability\n",
                "        \"\"\"\n",
                "\n",
                "        r = random.random()\n",
                "        if r < self.epsilon:\n",
                "            return np.random.choice(self.action_size)\n",
                "\n",
                "        return self.get_best_action(state)\n",
                "    \n",
                "  \n",
                "    def get_best_action(self, state):\n",
                "        \"\"\"\n",
                "        Compute the best action to take in a state.\n",
                "        \"\"\"\n",
                "\n",
                "        # self.model.eval()\n",
                "        # with torch.no_grad():\n",
                "        logits = self.model(torch.tensor(state, dtype=torch.float32)).tolist()\n",
                "\n",
                "        best_actions = []\n",
                "        score = float('-inf')\n",
                "\n",
                "        for i, logit in enumerate(logits):\n",
                "            if logit > score:\n",
                "                score = logit\n",
                "                best_actions = [i]\n",
                "            elif logit == score:\n",
                "                best_actions.append(i)\n",
                "\n",
                "        return random.choice(best_actions)\n",
                "\n",
                "    def replay(self, batch_size):\n",
                "        \"\"\"\n",
                "        Function learn network using randomly selected actions from the memory. \n",
                "        First calculates Q value for the next state and choose action with the biggest value.\n",
                "        Target value is calculated according to:\n",
                "                Q(s,a) := (r + gamma * max_a(Q(s', a)))\n",
                "        except the situation when the next action is the last action, in such case Q(s, a) := r.\n",
                "        In order to change only those weights responsible for chosing given action, the rest values should be those\n",
                "        returned by the network for state state.\n",
                "        The network should be trained on batch_size samples.\n",
                "        \"\"\"\n",
                "\n",
                "        batch = random.choices(self.memory, k=batch_size)\n",
                "        self.model.train(True)\n",
                "        self.optimizer.zero_grad()\n",
                "\n",
                "        states, actions, rewards, next_states, dones = zip(*batch)\n",
                "\n",
                "        states_tensor = torch.tensor(states, dtype=torch.float32)\n",
                "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
                "        next_states_tensor = torch.tensor(next_states, dtype=torch.float32)\n",
                "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
                "\n",
                "        logits = self.model(states_tensor)\n",
                "        targets = torch.clone(logits)\n",
                "        next_state_outputs = self.model(next_states_tensor)\n",
                "        targets[list(range(len(batch))), actions] = rewards_tensor + self.gamma*torch.amax(next_state_outputs, dim=-1)*(1-dones_tensor)\n",
                "        loss = self.loss_fn(logits, targets)\n",
                "\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "\n",
                "\n",
                "    def update_epsilon_value(self):\n",
                "        #Every each epoch epsilon value should be updated according to equation: \n",
                "        #self.epsilon *= self.epsilon_decay, but the updated value shouldn't be lower then epsilon_min value\n",
                "        if self.epsilon*self.epsilon_decay < self.epsilon_min:\n",
                "            self.epsilon = self.epsilon_min\n",
                "        else:\n",
                "            self.epsilon *= self.epsilon_decay\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[0.6097, 0.9591, 0.9976]]),\n",
                            " tensor([[[0.5496, 0.1246, 0.4669, 0.6097],\n",
                            "          [0.2861, 0.9591, 0.6412, 0.7501],\n",
                            "          [0.9976, 0.7120, 0.4607, 0.7388]]]))"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "v = torch.rand((1, 3, 4))\n",
                "torch.amax(v, dim=-1), v"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Czas przygotowaÄ model sieci, ktĂłra bÄdzie siÄ uczyĹa poruszania po Ĺrodowisku _FrozenLake_, warstwa wejĹciowa powinna mieÄ tyle neuronĂłw ile jest moĹźlliwych stanĂłw, warstwa wyjĹciowa tyle neuronĂłw ile jest moĹźliwych akcji do wykonania:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = frozenLake(\"8x8\")\n",
                "\n",
                "state_size = env.get_number_of_states()\n",
                "action_size = len(env.get_possible_actions(None))\n",
                "learning_rate = 1e-3\n",
                "\n",
                "torch.random.manual_seed(42)\n",
                "model = nn.Sequential(\n",
                "    nn.Linear(state_size, 64),\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(64, action_size),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Czas nauczyÄ agenta poruszania siÄ po Ĺrodowisku _FrozenLake_, jako stan przyjmij wektor o liczbie elementĂłw rĂłwnej liczbie moĹźliwych stanĂłw, z wartoĹciÄ 1 ustawionÄ w komĂłrce o indeksie rĂłwnym aktualnemu stanowi, pozostaĹe elementy majÄ byÄ wypeĹnione zerami:\n",
                "\n",
                "- 1 pkt < 35 epok,\n",
                "- 0.5 pkt < 60 epok,\n",
                "- 0.25 pkt - w pozostaĹych przypadkach.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch #0\tmean reward = 0.000\tepsilon = 0.550\n",
                        "epoch #1\tmean reward = 0.000\tepsilon = 0.495\n",
                        "epoch #2\tmean reward = 0.000\tepsilon = 0.446\n",
                        "epoch #3\tmean reward = 0.000\tepsilon = 0.401\n",
                        "epoch #4\tmean reward = 0.000\tepsilon = 0.361\n",
                        "epoch #5\tmean reward = 0.000\tepsilon = 0.325\n",
                        "epoch #6\tmean reward = 0.000\tepsilon = 0.292\n",
                        "epoch #7\tmean reward = 0.000\tepsilon = 0.263\n",
                        "epoch #8\tmean reward = 0.000\tepsilon = 0.237\n",
                        "epoch #9\tmean reward = 0.000\tepsilon = 0.213\n",
                        "epoch #10\tmean reward = 0.000\tepsilon = 0.192\n",
                        "epoch #11\tmean reward = 0.010\tepsilon = 0.173\n",
                        "epoch #12\tmean reward = 0.010\tepsilon = 0.155\n",
                        "epoch #13\tmean reward = 0.030\tepsilon = 0.140\n",
                        "epoch #14\tmean reward = 0.250\tepsilon = 0.126\n",
                        "epoch #15\tmean reward = 0.760\tepsilon = 0.113\n",
                        "epoch #16\tmean reward = 0.870\tepsilon = 0.102\n",
                        "epoch #17\tmean reward = 0.900\tepsilon = 0.092\n",
                        "epoch #18\tmean reward = 0.890\tepsilon = 0.083\n",
                        "epoch #19\tmean reward = 0.920\tepsilon = 0.074\n",
                        "You Win!\n"
                    ]
                }
            ],
            "source": [
                "from copy import copy\n",
                "\n",
                "agent = DQNAgent(action_size, learning_rate, model)\n",
                "agent.epsilon = 0.55\n",
                "done = False\n",
                "batch_size = 128\n",
                "EPISODES = 10000\n",
                "counter = 0\n",
                "for e in range(EPISODES):\n",
                "\n",
                "    summary = []\n",
                "    for _ in range(100):\n",
                "        total_reward = 0\n",
                "        env_state = env.reset()\n",
                "    \n",
                "        #\n",
                "        # INSERT CODE HERE to prepare appropriate format of the state for network\n",
                "        #\n",
                "\n",
                "        state = [0]*state_size\n",
                "        state[env_state] = 1.\n",
                "\n",
                "        for time in range(1000):\n",
                "            action = agent.get_action(state)\n",
                "            next_state_env, reward, done, _ = env.step(action)\n",
                "            total_reward += reward\n",
                "\n",
                "            #\n",
                "            # INSERT CODE HERE to prepare appropriate format of the next state for network\n",
                "            #\n",
                "            next_state = [0]*state_size\n",
                "            next_state[next_state_env] = 1.\n",
                "\n",
                "            #add to experience memory\n",
                "            agent.remember(state, action, reward, next_state, done)\n",
                "            state = copy(next_state)\n",
                "            if done:\n",
                "                break\n",
                "\n",
                "        #\n",
                "        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n",
                "        #\n",
                "        \n",
                "        if len(agent.memory) > batch_size:\n",
                "            agent.replay(batch_size)\n",
                "        \n",
                "        summary.append(total_reward)\n",
                "    \n",
                "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))\n",
                "    if np.mean(summary) > 0.9:\n",
                "        print (\"You Win!\")\n",
                "        break\n",
                "    \n",
                "    agent.update_epsilon_value()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Czas przygotowaÄ model sieci, ktĂłra bÄdzie siÄ uczyĹa poruszania po Ĺrodowisku _FrozenLakeExtended_, tym razem stan nie jest okreĹlany poprzez pojedynczÄ liczbÄ, a przez 3 tablice:\n",
                "\n",
                "- pierwsza zawierajÄca informacje o celu,\n",
                "- druga zawierajÄca informacje o dziurach,\n",
                "- trzecia zawierajÄca informacjÄ o poĹoĹźeniu gracza.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = frozenLakeExtended(\"4x4\")\n",
                "\n",
                "state_size = env.get_number_of_states()\n",
                "action_size = len(env.get_possible_actions(None))\n",
                "learning_rate = 0.001\n",
                "\n",
                "class Model(nn.Module):\n",
                "    def __init__(self, input_dim, output_dim, hidden_dim=16):\n",
                "        super().__init__()\n",
                "        self.lin_in1 = nn.Linear(input_dim, hidden_dim)\n",
                "        self.lin_in2 = nn.Linear(input_dim, hidden_dim)\n",
                "        self.lin_in3 = nn.Linear(input_dim, hidden_dim)\n",
                "        self.relu = nn.ReLU()\n",
                "\n",
                "        self.lin = nn.Linear(hidden_dim*3, hidden_dim)\n",
                "        self.out_lin = nn.Linear(hidden_dim, output_dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        if len(x.shape) > 2:\n",
                "            x1 = self.relu(self.lin_in1(x[:, -3]))\n",
                "            x2 = self.relu(self.lin_in1(x[:, -2]))\n",
                "            x3 = self.relu(self.lin_in1(x[:, -1]))\n",
                "        else:\n",
                "            x1 = self.relu(self.lin_in1(x[-3]))\n",
                "            x2 = self.relu(self.lin_in1(x[-2]))\n",
                "            x3 = self.relu(self.lin_in1(x[-1]))\n",
                "\n",
                "        x = torch.concat([x1, x2, x3], dim=-1)\n",
                "        x = self.relu(self.lin(x))\n",
                "        return self.out_lin(x)\n",
                "    \n",
                "model = Model(state_size, action_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Czas nauczyÄ agenta poruszania siÄ po Ĺrodowisku _FrozenLakeExtended_, jako stan przyjmij wektor skĹadajÄcy siÄ ze wszystkich trzech tablic (2 pkt.):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch #0\tmean reward = 0.020\tepsilon = 0.750\n",
                        "epoch #1\tmean reward = 0.020\tepsilon = 0.675\n",
                        "epoch #2\tmean reward = 0.020\tepsilon = 0.608\n",
                        "epoch #3\tmean reward = 0.060\tepsilon = 0.547\n",
                        "epoch #4\tmean reward = 0.120\tepsilon = 0.492\n",
                        "epoch #5\tmean reward = 0.350\tepsilon = 0.443\n",
                        "epoch #6\tmean reward = 0.510\tepsilon = 0.399\n",
                        "epoch #7\tmean reward = 0.590\tepsilon = 0.359\n",
                        "epoch #8\tmean reward = 0.670\tepsilon = 0.323\n",
                        "epoch #9\tmean reward = 0.640\tepsilon = 0.291\n",
                        "epoch #10\tmean reward = 0.720\tepsilon = 0.262\n",
                        "epoch #11\tmean reward = 0.640\tepsilon = 0.235\n",
                        "epoch #12\tmean reward = 0.770\tepsilon = 0.212\n",
                        "epoch #13\tmean reward = 0.750\tepsilon = 0.191\n",
                        "epoch #14\tmean reward = 0.830\tepsilon = 0.172\n",
                        "You Win!\n"
                    ]
                }
            ],
            "source": [
                "agent = DQNAgent(action_size, learning_rate, model)\n",
                "\n",
                "agent.epsilon = 0.75\n",
                "\n",
                "done = False\n",
                "batch_size = 64\n",
                "EPISODES = 2000\n",
                "counter = 0\n",
                "for e in range(EPISODES):\n",
                "    summary = []\n",
                "    for _ in range(100):\n",
                "        total_reward = 0\n",
                "        env_state = env.reset()\n",
                "\n",
                "        state = env_state\n",
                "                \n",
                "        for time in range(1000):\n",
                "            action = agent.get_action(state)\n",
                "            next_state_env, reward, done, _ = env.step(action)\n",
                "            total_reward += reward\n",
                "\n",
                "            #\n",
                "            # INSERT CODE HERE to prepare appropriate format of the next state for network\n",
                "            #\n",
                "\n",
                "            next_state = next_state_env\n",
                "\n",
                "            #add to experience memory\n",
                "            agent.remember(state, action, reward, next_state, done)\n",
                "            state = next_state.copy()\n",
                "            if done:\n",
                "                break\n",
                "\n",
                "        #\n",
                "        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n",
                "        #\n",
                "        if len(agent.memory) > batch_size:\n",
                "            agent.replay(batch_size)\n",
                "        \n",
                "        summary.append(total_reward)\n",
                "    if np.mean(summary) > 0.9:\n",
                "        print (\"You Win!\")\n",
                "        break\n",
                "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))\n",
                "    agent.update_epsilon_value()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Czas przygotowaÄ model sieci, ktĂłra bÄdzie siÄ uczyĹa dziaĹania w Ĺrodowisku [_CartPool_](https://gym.openai.com/envs/CartPole-v0/):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = gym.make(\"CartPole-v0\").env\n",
                "state_size = env.observation_space.shape[0]\n",
                "action_size = env.action_space.n\n",
                "learning_rate = 0.001\n",
                "\n",
                "model = nn.Sequential(\n",
                "    nn.Linear(state_size, 64),\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(64, 32),\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(32, action_size)\n",
                ")\n",
                "\n",
                "# model = nn.Sequential(\n",
                "#     nn.Linear(state_size, 16),\n",
                "#     nn.ReLU(),\n",
                "#     nn.Linear(16, action_size)\n",
                "# )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Czas nauczyÄ agenta gry w Ĺrodowisku _CartPool_:\n",
                "\n",
                "- 1 pkt < 10 epok,\n",
                "- 0.5 pkt < 20 epok,\n",
                "- 0.25 pkt - w pozostaĹych przypadkach.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch #0\tmean reward = 13.280\tepsilon = 0.400\n",
                        "epoch #1\tmean reward = 15.250\tepsilon = 0.260\n",
                        "epoch #2\tmean reward = 24.540\tepsilon = 0.169\n",
                        "epoch #3\tmean reward = 44.610\tepsilon = 0.110\n",
                        "epoch #4\tmean reward = 57.110\tepsilon = 0.071\n",
                        "epoch #5\tmean reward = 151.640\tepsilon = 0.046\n",
                        "epoch #6\tmean reward = 142.120\tepsilon = 0.030\n",
                        "epoch #7\tmean reward = 177.610\tepsilon = 0.020\n",
                        "You Win!\n"
                    ]
                }
            ],
            "source": [
                "agent = DQNAgent(action_size, learning_rate, model)\n",
                "\n",
                "agent.epsilon = 0.4\n",
                "agent.epsilon_decay = 0.65\n",
                "\n",
                "done = False\n",
                "batch_size = 256\n",
                "EPISODES = 1000\n",
                "counter = 0\n",
                "for e in range(EPISODES):\n",
                "    summary = []\n",
                "    for _ in range(100):\n",
                "        total_reward = 0\n",
                "        env_state = env.reset()\n",
                "\n",
                "        state = env_state[0]\n",
                "\n",
                "        for time in range(300):\n",
                "            action = agent.get_action(state)\n",
                "            next_state_env, reward, done, _, _ = env.step(action)\n",
                "            total_reward += reward\n",
                "\n",
                "            #\n",
                "            # INSERT CODE HERE to prepare appropriate format of the next state for network\n",
                "            #\n",
                "            next_state = next_state_env\n",
                "            #add to experience memory\n",
                "            agent.remember(state, action, reward, next_state, done)\n",
                "            state = next_state.copy()\n",
                "            if done:\n",
                "                break\n",
                "\n",
                "        #\n",
                "        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n",
                "        #\n",
                "        if len(agent.memory) > batch_size:\n",
                "            agent.replay(batch_size)\n",
                "        \n",
                "        summary.append(total_reward)\n",
                "    if np.mean(summary) > 195:\n",
                "        print (\"You Win!\")\n",
                "        break\n",
                "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))\n",
                "    agent.update_epsilon_value()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
